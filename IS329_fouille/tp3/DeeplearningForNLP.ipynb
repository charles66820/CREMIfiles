{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning pour le NLP\n",
    "\n",
    "- Les méthodes Bag Of Words étaient à l'état de l'art jusqu'en 2013:\n",
    "\n",
    "    On a vu la méthode TF-IDF par exemple\n",
    "    \n",
    "- Puis est venu l'age d'or des méthodes de type Word Embedding:\n",
    "\n",
    "    On a vu Word2Vec mais on peut aussi citer fastText et GloVe\n",
    "\n",
    "    Word2Vec utilise des réseaux de neurones peu profond (1 couche cachée)\n",
    "    \n",
    "    Ces méthodes capturent le sens des mots, mais comment l'utiliser sur des phrases?\n",
    "    \n",
    "    Plusieures évolutions type Doc2Vec\n",
    "  \n",
    "- Le traitement des images utilise depuis longtemps et avec succès des réseaux profonds, pourquoi pas sur du texte?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les réseaux convolutionnels\n",
    "\n",
    "C'est des réseaux qui marchent très bien sur les images.\n",
    "\n",
    "![](https://i.stack.imgur.com/NU7y2.png)\n",
    "\n",
    "Ca tire parti du fait que des pixels proches dans une images sont \"liés\". \n",
    "\n",
    "Les convolutions vont extraires des features locales de ces groupes de pixels proches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mais sur du texte \n",
    "\n",
    "Ca ne marche pas pareil.\n",
    "\n",
    "Déjà on n'est déjà pas en 2D.\n",
    "\n",
    "On a une suite de mots qui se lisent les uns à la suite des autres. Les mots que l'on a lu depuis le début de la phrase nous aident à comprendre les mots suivants.\n",
    "\n",
    "2 mots proches vont comme les pixels d'une image être liés, c'est l'hypothèse que l'on a utilisé pour développer Word2Vec. Mais certains mots dans un texte vont faire référence à des mots très lointains.\n",
    "\n",
    "`L'article que j'ai lu était très intéressant, il parlait de NLP`\n",
    "\n",
    "Le **il** dans cette phrase fait référence à **article** qui est 9 mots avant lui."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1ère architecture : des réseaux de neurones récurrents\n",
    "\n",
    "Les 1ères architectures utilisées avec succes sur du texte étaient des réseaux de neurones récurrents.\n",
    "\n",
    "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)\n",
    "\n",
    "On propage les mots d'un texte 1 à 1 dans ce type de réseau, le réseau calcule un output $h$ que l'on appelle aussi \"hidden state\" et qui va être utilisé par le réseau lors de l'insertion du prochain mot.\n",
    "\n",
    "Cet état caché représente le contexte de la phrase que l'on traite.\n",
    "\n",
    "Ces réseaux sont très difficiles à entrainer à cause du problème dit du \"exploding/vanishing gradient\".\n",
    "\n",
    "En pratique le contexte ne permet de faire référence qu'à des mots très proches du mot courant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Le RNN qui sauve la donne : LSTM\n",
    "\n",
    "Une architecture a été développée pour traiter ce problème et être capable de capturer des dépendances longue distance entre des mots. \n",
    "\n",
    "Les **Long-Short Term Memory**.\n",
    "\n",
    "![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png)\n",
    "\n",
    "Une LSTM est composée de 3 \"portes\" qui contrôle l'état de la cellule (le \"hidden state\"):\n",
    "\n",
    "- Une porte d'oubli\n",
    "- Une porte d'entrée\n",
    "- Une porte de sortie\n",
    "\n",
    "La porte d'oubli va permettre de vider la cellule.\n",
    "\n",
    "La porte d'entrée va permettre de mettre à jour l'état de la cellule\n",
    "\n",
    "La porte de sortie va calculer l'output du réseau en fonction de l'état de la cellule et du signal en entrée. \n",
    "\n",
    "## Forces et faiblesses :\n",
    "- On arrive à capturer des dépendances à plus long terme\n",
    "- Mais pas si loin quand même\n",
    "- Plus on avance dans la phrase mieux le contexte est capturé\n",
    "- Mais du coup en début de phrase on n'est pas très bon\n",
    "- Quand on traite un mot, on connait le début de la phrase mais rien de la fin de la phrase, il peut nous manquer de l'information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's go deeper : Le mécanisme d'Attention\n",
    "\n",
    "C'est un mécanisme qui permet de capturer les dépendances termes à termes entre 2 sequences.\n",
    "\n",
    "Très utile pour de la traduction par exemple:\n",
    "\n",
    "![](https://wiki.pathmind.com/images/wiki/attention_translation_grid.png)\n",
    "\n",
    "Les maths derrière ce mécanisme : \n",
    "$$\\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "Si les 2 phrases sont identiques, ca marche aussi!\n",
    "\n",
    "On parle de **self-attention**.\n",
    "\n",
    "Autre gros atout de ce mécanisme, on traite la phrase dans son ensemble, on capture réellement des dépendances à long terme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention is All You Need : Le Transformer\n",
    "\n",
    "Dans le papier [Attention is All You Need](https://arxiv.org/abs/1706.03762) les auteurs proposent une nouvelle architecture qui utilise de nombreuses mécaniques d'Attention.\n",
    "\n",
    "Le Transformer :\n",
    "![](https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png)\n",
    "\n",
    "![](https://jalammar.github.io/images/t/Transformer_decoder.png)\n",
    "\n",
    "__Ressources__ :\n",
    "\n",
    "The Illustrated Transformer : http://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "The Annotated Transformer : https://nlp.seas.harvard.edu/2018/04/03/attention.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 modèles à base de transformers qui ont fait parler d'eux \n",
    "\n",
    "- BERT : Bidirectional Encoder Representations from Transformers [Open sourcé](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html) par google en Novembre 2018 et utilisé dans google search depuis novembre 2019\n",
    "\n",
    "![](https://jalammar.github.io/images/distilBERT/bert-distilbert-sentence-classification.png)\n",
    "\n",
    "BERT est entrainé sur 2 taches :\n",
    "\n",
    "- Prédire des mots qui ont été masqués dans le texte.\n",
    "- Prédire des relations entre 2 phrases consécutives.\n",
    "\n",
    "Les performances de BERT :\n",
    "\n",
    "- Sur le problème SQuAD (Stanford Question Answering Dataset):\n",
    "![](https://4.bp.blogspot.com/-iQZIsE3lbVY/W9i8Tc-F7RI/AAAAAAAADfU/DrxjBoDfqrwe6GJUxENqWuzQ0IPlgT3TgCLcBGAs/s1600/image3.png)\n",
    "\n",
    "- Sur le benchmark GLUE qui regroupe 9 taches de NLU BERT surpasse de 7.6% l'ancien meilleur modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C'est encore en Anglais :'(\n",
    "\n",
    "Mais merci l'INRIA qui a publié ...\n",
    "\n",
    "# CamemBERT: [A Tasty French Language Model](https://arxiv.org/abs/1911.03894)\n",
    "\n",
    "![](https://camembert-model.fr/authors/admin/avatar_huac8a9374dbd7d6a2cb77224540858ab4_463389_250x250_fill_lanczos_center_2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Et le 2ème modèle qui a marqué les esprits récemment: GPT-2 (3)\n",
    "\n",
    "Les articles dans la presse\n",
    "\n",
    "- \"le générateur de texte trop performant pour être rendu public\"\n",
    "- \"cette intelligence artificielle pourrait devenir l'arme ultime des trolls\"\n",
    "\n",
    "Les informations données par OpenAI qui a développé ce modèle :\n",
    "\n",
    "`Release Strategy\n",
    "Due to concerns about large language models being used to generate deceptive, biased, or abusive language at scale, we are only releasing a much smaller version of GPT-2 along with sampling code. We are not releasing the dataset, training code, or GPT-2 model weights`\n",
    "\n",
    "Mais 9 mois plus tard : [La release](https://www.openai.com/blog/gpt-2-1-5b-release/) du modèle 1.5 Milliards de paramètres.\n",
    "\n",
    "## L'entrainement :\n",
    "GPT-2 est entrainé sur une tache simple : A partir d'une phrase prédire le mot suivant\n",
    "\n",
    "Mais tout se joue au niveau de la démesure:\n",
    "\n",
    " - 40GB de texte provenant d'internet\n",
    " - 8 millions de pages Web\n",
    " - 1.5 Milliards de paramètres\n",
    " \n",
    "## Les résultats :\n",
    "\n",
    "Des performances qui surpassent l'état de l'art dans plusieurs domaines:\n",
    "- Compréhension de texte\n",
    "- Résumé de texte\n",
    "- Réponse à des questions\n",
    "- Traduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Mais c'est super complexe !\n",
    "\n",
    "Les modèles sont d'une complexité de plus en plus importante.\n",
    "\n",
    "Mais il est très très facile de les utiliser.\n",
    "\n",
    "HuggingFace propose énormément d'implémentations de modèles Etat de l'Art à base de [transformers](https://huggingface.co/transformers/)\n",
    "![](https://huggingface.co/landing/assets/transformers-docs/huggingface_logo.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Et en 2022 ?\n",
    "\n",
    "Techniques :\n",
    "https://openai.com/blog/techniques-for-training-large-neural-networks/\n",
    "\n",
    "Recherche d'information :\n",
    "https://openai.com/blog/webgpt/ (bon ok c'est 2021...)\n",
    "\n",
    "Des maths : \n",
    "https://github.com/facebookresearch/SymbolicMathematics (bon là c'est 2020)\n",
    "\n",
    "Chat : \n",
    "https://openai.com/blog/chatgpt/\n",
    "\n",
    "Text + Image :\n",
    "https://openai.com/blog/dall-e-2-extending-creativity/ ou https://stablediffusionweb.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
