{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap sur le modèle Bag Of Words\n",
    " - Un texte est modélisé comme un ensemble non ordonné de mots\n",
    " - Chaque mot va être représenté par un vecteur avec la technique du One-Hot Encoding\n",
    " - On peut créer différentes variations du modèle en comptant les mots de façon + ou - complexe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les forces du modèle Bag Of Words\n",
    "- Il est simple!\n",
    "- Il s'adapte à n'importe quelle langue et à n'importe quel vocabulaire\n",
    "- Il y a beaucoup de variants : TF, TF-IDF, BM25\n",
    "- On peut y rajouter de l'information en utilisant des N-grams\n",
    "- C'est une méthode ancienne : Beaucoup de litérature sur le sujet, et des implémentations dans tout les langages de programmation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Les limites du modèle Bag of Words\n",
    "- La taille des vecteurs générés grossit avec la taille du vocabulaire \n",
    "- L'ajout des N-Grams fait exploser cette croissance\n",
    "- Les vecteurs générés sont très sparses ça pose problème aux méthodes de machine learning qui les utilisent\n",
    "- Des mots similaires ont une représentation différente\n",
    "- C'est un peu triste de faire juste du comptage de mots...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les embeddings vectoriels à la rescousse\n",
    "\n",
    "On va conserver la même approche, on veut convertir un mot en vecteur, mais sans les désagréments du modèle BoW.\n",
    "\n",
    "La promesse des embeddings vectoriels : \n",
    "- Chaque mot va être représenté par un *vecteur dense* de taille *raisonnable*\n",
    "- Ce vecteur va être porteur de sens (au sens sémantique)\n",
    "\n",
    "Ok, mais comment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Le modèle qui a changé la donne : Word2Vec\n",
    "```\n",
    "Word2vec was created and published in 2013 by a team of researchers led by Tomas Mikolov at Google\n",
    "```\n",
    "https://en.wikipedia.org/wiki/Word2vec\n",
    "\n",
    "## L'idée de base : exploiter l'hypothèse distributionnelle\n",
    "```\n",
    "Hypothèse selon laquelle les mots qui apparaissent dans les mêmes contextes linguistiques partagent des significations similaires.\n",
    "````\n",
    "\n",
    "Dans un texte, un mot va souvent être entouré des mêmes termes.\n",
    "\n",
    "Le contexte d'un mot c'est les mots qui entournent ce mot. On va le choisir comme les *n* termes qui précèdent le mot que l'on traite et les *n* termes qui le suivent.\n",
    "\n",
    "### Illustration avec des brèves de comptoir : [wikipedia](https://fr.wikipedia.org/wiki/Br%C3%A8ves_de_comptoir#Exemples)\n",
    "```\n",
    "« À la naissance le nain est normal, c'est en grandissant qu'il rapetisse. »\n",
    "```\n",
    "Contexte pour **nain** de taille 4 : [À, la, naissance, le, est, normal, c', est] \n",
    "\n",
    "```\n",
    "« Les auteurs modernes font des livres tellement petits qu'on ne peut plus mettre des fleurs à sécher dedans. »\n",
    "```\n",
    "Contexte pour **livres** de taille 5 : [Les, auteurs, modernes, font, des, tellement, petits, qu',on, ne]\n",
    "\n",
    "## Et on en fait quoi de cette idée ?\n",
    "\n",
    "Si on arrive à faire prédire le contexte d'un mot grace à un modèle de machine learning, c'est que le modèle aurra réussi à capturer la signification du mot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Promesse tenue?\n",
    "\n",
    "Google a publié un modèle Word2Vec : https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "```\n",
    "The model was trained on the Google News dataset (about 100 billion words). \n",
    "The model contains 300-dimensional vectors for 3 million words and phrases.\n",
    "```\n",
    "\n",
    "Le résultat marquant :\n",
    "$$\\vec{V}_{King} -\\vec{V}_{Man} + \\vec{V}_{Woman} \\approx \\vec{V}_{Queen}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.3 s, sys: 2.8 s, total: 49.1 s\n",
      "Wall time: 48.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import gensim\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    '/mnt/data/GoogleNews-vectors-negative300.bin.gz',\n",
    "    binary=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118193507194519),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431011199951),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377321839332581),\n",
       " ('kings', 0.5236844420433044),\n",
       " ('Queen_Consort', 0.5235945582389832),\n",
       " ('queens', 0.5181134343147278),\n",
       " ('sultan', 0.5098593831062317),\n",
       " ('monarchy', 0.5087411999702454)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "queen = w2v[\"king\"] - w2v[\"man\"] + w2v[\"woman\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', 0.8449392318725586),\n",
       " ('queen', 0.7300517559051514),\n",
       " ('monarch', 0.645466148853302),\n",
       " ('princess', 0.6156251430511475),\n",
       " ('crown_prince', 0.5818676352500916),\n",
       " ('prince', 0.5777117609977722),\n",
       " ('kings', 0.5613663792610168),\n",
       " ('sultan', 0.5376775860786438),\n",
       " ('Queen_Consort', 0.5344247817993164),\n",
       " ('queens', 0.5289887189865112)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=[queen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "queen_norm = (\n",
    "    w2v.get_vector(\"king\", norm=True)\n",
    "    - w2v.get_vector(\"man\", norm=True)\n",
    "    + w2v.get_vector(\"woman\", norm=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.00524902, -0.14355469, -0.06933594,  0.12353516,  0.13183594,\n",
       "        -0.08886719, -0.07128906, -0.21679688, -0.19726562,  0.05566406],\n",
       "       dtype=float32),\n",
       " array([ 0.04296875, -0.17822266, -0.12908936,  0.11523438,  0.00268555,\n",
       "        -0.10229492,  0.19580078, -0.1795044 ,  0.01953125,  0.40991974],\n",
       "       dtype=float32),\n",
       " array([-0.00619015, -0.07542217, -0.05083442,  0.04359096, -0.00321992,\n",
       "        -0.035044  ,  0.08129873, -0.06163806, -0.00235739,  0.14262468],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v[\"queen\"][:10], queen[:10], queen_norm[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', 0.7992597818374634),\n",
       " ('queen', 0.7118192911148071),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431011199951),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377321839332581),\n",
       " ('kings', 0.5236844420433044),\n",
       " ('Queen_Consort', 0.5235945582389832),\n",
       " ('queens', 0.518113374710083),\n",
       " ('sultan', 0.5098593831062317)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=[queen_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('puppy', 0.672011137008667),\n",
       " ('pooch', 0.640053927898407),\n",
       " ('pup', 0.612525224685669),\n",
       " ('dogs', 0.6005871295928955),\n",
       " ('German_shepherd', 0.591395378112793),\n",
       " ('cat', 0.5822558999061584),\n",
       " ('golden_retriever', 0.5778035521507263),\n",
       " ('pit_bull', 0.5748783349990845),\n",
       " ('Rottweiler', 0.5717918276786804),\n",
       " ('Yorkshire_terrier', 0.5693223476409912)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['dog', 'child'], negative=['adult'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('puppy', 0.6052532196044922),\n",
       " ('kitten', 0.6021385788917542),\n",
       " ('dog', 0.5852227807044983),\n",
       " ('pup', 0.5823509693145752),\n",
       " ('pooch', 0.5800433158874512),\n",
       " ('cats', 0.5533753633499146),\n",
       " ('stray_cat', 0.5414088368415833),\n",
       " ('kitties', 0.5325700044631958),\n",
       " ('Yorkshire_terrier', 0.5320756435394287),\n",
       " ('pet', 0.5289697647094727)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['cat', 'child'], negative=['adult'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woof_woof', 0.5396372079849243),\n",
       " ('dogs', 0.4995119869709015),\n",
       " ('bark_incessantly', 0.4923662543296814),\n",
       " ('Woof_woof', 0.4894609749317169),\n",
       " ('Rusty_barked', 0.4648567736148834),\n",
       " ('woofs', 0.46396562457084656),\n",
       " ('yip_yip', 0.45828351378440857),\n",
       " ('growl', 0.45362916588783264),\n",
       " ('moos', 0.4518892765045166),\n",
       " ('barking_incessantly', 0.45174774527549744)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['meow', 'dog'], negative=[\"cat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Milan', 0.7222141623497009),\n",
       " ('Rome', 0.702830970287323),\n",
       " ('Palermo_Sicily', 0.5967570543289185),\n",
       " ('Italian', 0.5911272764205933),\n",
       " ('Tuscany', 0.5632812976837158),\n",
       " ('Bologna', 0.5608358383178711),\n",
       " ('Sicily', 0.5596384406089783),\n",
       " ('Bologna_Italy', 0.5470058917999268),\n",
       " ('Berna_Milan', 0.5464027523994446),\n",
       " ('Genoa', 0.5308900475502014)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['Paris', 'Italy'], negative=[\"France\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Friuli', 0.6579111814498901),\n",
       " ('Lecce', 0.6504460573196411),\n",
       " ('Milan', 0.6464554667472839),\n",
       " ('Livorno', 0.6451722979545593),\n",
       " ('Cagliari', 0.6402646899223328),\n",
       " ('Brescia', 0.6326268315315247),\n",
       " ('Bologna', 0.6313794851303101),\n",
       " ('Genoa', 0.6303693652153015),\n",
       " ('Tuscany', 0.6288278698921204),\n",
       " ('Palermo', 0.6284286975860596)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['Bordeaux', 'Italy'], negative=[\"France\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Falanghina', 0.6475211977958679),\n",
       " ('Chianti', 0.6175957918167114),\n",
       " ('Chianti_Classico', 0.6059287786483765),\n",
       " ('Barolo', 0.6030843257904053),\n",
       " (\"Nero_d'_Avola\", 0.602142333984375),\n",
       " ('barbera', 0.6005603075027466),\n",
       " ('Pinot_Grigio', 0.5974807143211365),\n",
       " ('Valpolicella', 0.5972704887390137),\n",
       " ('Riserva', 0.5970005393028259),\n",
       " ('malvasia', 0.5907768607139587)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['bordeaux', 'Italy'], negative=[\"France\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pinotage', 0.5918348431587219),\n",
       " ('Chenin', 0.5434677004814148),\n",
       " ('shiraz', 0.5277871489524841),\n",
       " ('Vergelegen', 0.5246654748916626),\n",
       " ('Pinotage', 0.5209116339683533),\n",
       " ('rooibos_tea', 0.5206111669540405),\n",
       " ('chenin', 0.5097901821136475),\n",
       " ('Cap_Classique', 0.5078455209732056),\n",
       " ('sauvignon', 0.507658839225769),\n",
       " ('Nederburg', 0.5071874260902405)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['bordeaux', 'South_Africa'], negative=[\"France\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Comment ça marche ?\n",
    " - Une prédiction : 2 possibilités\n",
    " - Architecture de réseau de neurone\n",
    " - La tâche fantôme\n",
    " - Principes d'entrainement\n",
    "\n",
    "On a 2 possibilités pour la prédiction : CBOW et Skip-Gram\n",
    "![Word2Vec](w2v.png)\n",
    "\n",
    "CBOW is faster while skip-gram does a better job for infrequent words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un réseau de neurones à 1 couche cachée pour faire le job\n",
    "![Architecture](https://israelg99.github.io/images/2017-03-23-Word2Vec-Explained/skip_gram_net_arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On avait parlé de générer des vecteurs !?\n",
    "Et on se retrouve avec un réseau de neurones\n",
    "\n",
    "## Magic trick :\n",
    "On va jetter purement et simplement la dernière couche.\n",
    "\n",
    "La couche cachée est en fait une matrice de taille $n_{vocab} \\times n_{hidden}$\n",
    "\n",
    "La couche d'entrée est alimentée par des mots représentés par des vecteurs \"One-Hot encoded\":\n",
    "\n",
    "![embed](https://israelg99.github.io/images/2017-03-23-Word2Vec-Explained/matrix_mult_w_one_hot.png)\n",
    "\n",
    "## Voilà notre embedding\n",
    "![MagicTrick](https://israelg99.github.io/images/2017-03-23-Word2Vec-Explained/word2vec_weight_matrix_lookup_table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Et pour entrainer le modèle on fait comment?\n",
    "\n",
    "On utilise une méthode appelée le **négative sampling**\n",
    "\n",
    "On génère tout d'abord des couples de mots avec le **mot à prédire** et **les mots du contexte**\n",
    "![Sampling](https://israelg99.github.io/images/2017-03-23-Word2Vec-Explained/training_data.png)\n",
    "\n",
    "On sample aussi **N mots** du vocabulaire **absent du contexte**.\n",
    "\n",
    "On va alors maximiser la métrique (log-likelihood) pour les mots du contexte et minimiser la métrique pour les mots hors contexte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Références :\n",
    "- https://arxiv.org/abs/1301.3781\n",
    "- https://en.wikipedia.org/wiki/Word2vec\n",
    "- https://israelg99.github.io/2017-03-23-Word2Vec-Explained/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP\n",
    "2 tâches : \n",
    " - Explorer le modèle Word2Vec publié par google\n",
    " - Utiliser le modèle Word2Vec pour faire de l'analyse de sentiment sur des critiques de films"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
